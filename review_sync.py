# -*- coding: utf-8 -*-
"""review_sync.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bJ9iNpWcCG1drpJfdPYZgwF0eJm9RGy

Install packages
"""

!pip install -q google-play-scraper snowflake-connector-python pandas tqdm

"""Import libraries"""

from google_play_scraper import reviews, Sort, app
import pandas as pd
import snowflake.connector
from datetime import datetime, timezone
from tqdm import tqdm
import time
import os

"""Define Snowflake connection parameters"""

conn_params = {
    "user": "USER1204",
    "password": "Review20251018@",
    "account": "XUZXIIE-EAC06737",
    "warehouse": "COMPUTE_WH",
    "database": "GPT_REVIEWS_DB",
    "schema": "PUBLIC",
    "role": "ACCOUNTADMIN"
}

"""Fetch reviews from Google Play"""

print("Fetching reviews from Google Play...")

app_id = "com.openai.chatgpt"
buf, token, first = [], None, True
pbar = tqdm(desc="Fetching", unit="reviews")

while True:
    if first:
        res, token = reviews(app_id, lang="en", country="us", sort=Sort.NEWEST, count=200)
        first = False
    else:
        if token is None:
            break
        res, token = reviews(app_id, continuation_token=token)

    if not res:
        break

    buf.extend(res)
    pbar.update(len(res))

    time.sleep(0.2)

pbar.close()
df = pd.DataFrame(buf)
print(f"Total reviews fetched: {len(df):,}")

"""Clean and format data"""

# Convert 'at' column to datetime
df['at'] = pd.to_datetime(df['at'], errors='coerce')

# Rename columns for clarity
df.rename(columns={
    "reviewId": "review_id",
    "userName": "user_name",
    "content": "content",
    "score": "score",
    "at": "created_at",
    "appVersion": "app_version"
}, inplace=True)

# Keep only necessary fields
df = df[[
    "review_id", "user_name", "content",
    "score", "created_at", "app_version"
]]

"""Upload to Snowflake"""

print("Uploading data to Snowflake (batch size = 200,000)...")

conn = snowflake.connector.connect(**conn_params)
cursor = conn.cursor()

# Step 1: Create target table if not exists
cursor.execute("""
CREATE TABLE IF NOT EXISTS reviews (
    review_id STRING PRIMARY KEY,
    user_name STRING,
    content TEXT,
    score INT,
    created_at TIMESTAMP,
    app_version STRING
)
""")

# Step 2: Create staging table (temporary)
cursor.execute("""
CREATE OR REPLACE TEMPORARY TABLE reviews_staging LIKE reviews;
""")

# Step 3: Prepare data
records = []
for _, row in df.iterrows():
    created_at = row["created_at"].strftime("%Y-%m-%d %H:%M:%S") if pd.notnull(row["created_at"]) else None
    records.append((
        row["review_id"],
        row["user_name"],
        row["content"],
        int(row["score"]) if pd.notnull(row["score"]) else None,
        created_at,
        row["app_version"]
    ))

print(f"Total records to insert: {len(records):,}")

# Step 4: Batch insert
BATCH_SIZE = 200000
insert_sql = """
INSERT INTO reviews_staging (
    review_id, user_name, content, score, created_at, app_version
) VALUES (%s, %s, %s, %s, %s, %s)
"""

for i in range(0, len(records), BATCH_SIZE):
    batch = records[i:i + BATCH_SIZE]
    print(f"Inserting batch {i // BATCH_SIZE + 1}: {len(batch):,} records...")
    cursor.executemany(insert_sql, batch)

# Step 5: Merge from staging to main table
print("Merging data into reviews table...")

merge_sql = """
MERGE INTO reviews AS target
USING reviews_staging AS source
ON target.review_id = source.review_id
WHEN MATCHED THEN UPDATE SET
    content = source.content,
    score = source.score,
    created_at = source.created_at,
    app_version = source.app_version
WHEN NOT MATCHED THEN INSERT (
    review_id, user_name, content, score, created_at, app_version
) VALUES (
    source.review_id, source.user_name, source.content,
    source.score, source.created_at, source.app_version
);
"""
cursor.execute(merge_sql)


# Finalize
conn.commit()
cursor.close()
conn.close()
print("\n Data upload completed.")

"""app_metadata Table"""

# Commented out IPython magic to ensure Python compatibility.
print("Fetching app metadata...")
conn = snowflake.connector.connect(**conn_params)
cursor = conn.cursor()

metadata = app(app_id, lang="en", country="us")
fetched_at = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")

# Prepare metadata mapping for your table schema
metadata_row = {
    "APP_VERSION": metadata.get("version"),
    "TITLE": metadata.get("title"),
    "DEVELOPER": metadata.get("developer", "OpenAI"),
    "GENRE": metadata.get("genre", "Productivity"),
    "SCORE": metadata.get("score"),
    "RATINGS_COUNT": metadata.get("ratings"),
    "REVIEWS_COUNT": metadata.get("reviews"),
    "INSTALLS": metadata.get("installs"),
    "REAL_INSTALLS": metadata.get("realInstalls"),
    "IS_FREE": metadata.get("free"),
    "PRICE": metadata.get("price"),
    "CURRENCY": metadata.get("currency"),
    "SALE": metadata.get("sale", False),
    "OFFERS_IAP": metadata.get("offersIAP"),
    "IAP_PRICE_RANGE": metadata.get("inAppProductPrice"),
    "URL": metadata.get("url", "https://play.google.com/store/apps/details?id=com.openai.chatgpt"),
    "FETCHED_AT": fetched_at
}

# Insert a new metadata record into existing table
cursor = conn.cursor()
cursor.execute("""
INSERT INTO APP_METADATA (
    APP_VERSION, TITLE, DEVELOPER, GENRE, SCORE, RATINGS_COUNT, REVIEWS_COUNT,
    INSTALLS, REAL_INSTALLS, IS_FREE, PRICE, CURRENCY, SALE, OFFERS_IAP,
    IAP_PRICE_RANGE, URL, FETCHED_AT
) VALUES (
#     %(APP_VERSION)s, %(TITLE)s, %(DEVELOPER)s, %(GENRE)s, %(SCORE)s, %(RATINGS_COUNT)s, %(REVIEWS_COUNT)s,
#     %(INSTALLS)s, %(REAL_INSTALLS)s, %(IS_FREE)s, %(PRICE)s, %(CURRENCY)s, %(SALE)s, %(OFFERS_IAP)s,
#     %(IAP_PRICE_RANGE)s, %(URL)s, %(FETCHED_AT)s
)
""", metadata_row)

cursor.close()
conn.close()

print("ChatGPT app metadata successfully inserted into APP_METADATA table.")